# JMeter-AI 项目说明文档

## 1. 项目简介

本项目是一个基于 **Spring Boot 3.2** 和 **大语言模型（LLM）** 的智能接口测试自动化工具。它旨在解决传统接口测试中用例编写繁琐、结果校验单一（仅靠状态码）的问题。

项目核心理念是 **"单接口多场景深度测试"**。它能够解析 Swagger/OpenAPI 文档，针对每个接口应用不同的 **质量场景（Quality Scenarios）**（如：快乐路径、参数完整性校验、异常输入测试等），自动生成测试用例并执行。

更重要的是，它引入了 **LLM 智能校验** 机制：测试结果不再仅凭 HTTP 状态码判断成功/失败，而是将请求、响应和接口定义一同回传给 LLM，由 LLM 根据业务逻辑判断测试是否通过。

## 2. 核心功能

*   **多模型支持**：
    *   底层使用 `openai-java` SDK (v0.15.0+)，兼容所有支持 OpenAI 协议的厂商。
    *   已适配 **DeepSeek (深度求索)**、**Google Gemini**、**阿里云 DashScope (通义千问)**。
    *   通过环境变量无缝切换模型提供商。
*   **场景化测试 (Scenario-Based Testing)**：
    *   摒弃粗放的批量生成，采用 **接口粒度** 的测试策略。
    *   针对每个接口，自动遍历预设的 **质量场景**：
        *   `HAPPY_PATH` (快乐路径): 正常业务流程，预期成功。
        *   `PARAM_INTEGRITY` (参数完整性): 缺少必选参数，预期失败。
        *   `ABNORMAL_INPUT` (异常输入): 非法数据格式/边界值，预期失败或报错。
*   **智能校验 (LLM Verification)**：
    *   **语义级断言**：LLM 分析响应体内容。例如：虽然返回 200 OK，但响应体包含 `{"code": 500, "msg": "系统错误"}`，LLM 会判定为测试失败。
    *   **预期行为验证**：如果测试目标是"测试参数缺失"，服务器返回 400 Bad Request，LLM 会判定测试 **通过**（符合预期）。
    *   **强制断言生成与重试机制**：系统强制要求 LLM 为每个用例生成基于业务逻辑的断言（不仅仅是状态码），如果生成失败会自动重试（默认 3 次），确保测试结果的严谨性，不再依赖宽松的默认通过策略。
*   **轻量级执行**：
    *   内置 `CurlExecutorService`，基于 OkHttp 高并发执行。
    *   生成可复现的 Curl 命令，方便开发者手动复核。
*   **结构化报告**：
    *   输出包含用例详情、执行 Curl、响应数据、LLM 校验理由的完整 JSON 报告。
    *   自动生成项目级中文测试总结。
*   **快速重跑 (Re-run without LLM)**：
    *   支持对已完成的任务进行“一键重跑”。
    *   **本地极速校验**：初次运行时，系统会根据 LLM 分析结果自动生成结构化断言（如状态码、JSON字段、响应时间等）。重跑时，直接使用这些断言在本地进行校验，**无需再次调用 LLM**，大幅降低成本并提升速度。

## 3. 技术架构

项目基于 Java 21 和 Spring Boot 3.2.3 开发。

*   **Web 框架**: Spring Boot Web
*   **核心逻辑**: 
    *   `PipelineService`: 核心调度器。负责 "Swagger解析 -> 遍历接口 -> 遍历场景 -> 生成用例 -> 执行用例 -> LLM校验 -> 汇总报告" 的全生命周期。
*   **LLM 服务**: 
    *   `LlmService`: 基于 `com.openai:openai-java` SDK 封装。支持动态配置 Base URL 和 Model，实现 "一次编码，随处调用"。
*   **执行引擎**: 
    *   `CurlExecutorService`: 使用 OkHttp 执行 HTTP 请求，自动处理 Header 和 Body，并记录详细的耗时和状态。
*   **数据模型**: 
    *   `QualityScenario` (Enum): 定义测试场景策略。
    *   `TestCase` / `ExecutionResult`: 结构化的用例和结果对象。
*   **Prompt 管理**:
    *   `PromptPresets`: 集中管理所有与 LLM 交互的提示词（System Prompt / User Prompt），方便调优。

## 4. 环境配置

项目完全通过环境变量配置，支持 `.env` 文件或系统环境变量。

### 基础配置
| 环境变量名 | 描述 | 默认值/示例 |
| :--- | :--- | :--- |
| `SERVER_PORT` | 服务端口 | `8080` |
| `LLM_PROVIDER` | LLM 提供商选择 | `deepseek` / `gemini` / `dashscope` |

### 模型厂商配置

**1. DeepSeek (推荐)**
| 变量名 | 描述 |
| :--- | :--- |
| `DEEPSEEK_API_KEY` | API Key |
| `DEEPSEEK_BASE_URL` | `https://api.deepseek.com` |
| `DEEPSEEK_MODEL` | `deepseek-chat` |

**2. Google Gemini**
| 变量名 | 描述 |
| :--- | :--- |
| `GEMINI_API_KEY` | API Key |
| `GEMINI_BASE_URL` | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| `GEMINI_MODEL` | `gemini-3-pro-preview` |

**3. 阿里云 DashScope**
| 变量名 | 描述 |
| :--- | :--- |
| `DASHSCOPE_API_KEY` | API Key |
| `DASHSCOPE_BASE_URL` | `https://dashscope.aliyuncs.com/compatible-mode/v1` |
| `DASHSCOPE_MODEL` | `qwen-plus` |

## 5. 构建与运行

### 5.1 构建项目

```bash
mvn clean package -DskipTests
```
产物路径: `target/jmeter-ai-0.2.0-SNAPSHOT.jar`

### 5.2 运行服务

```bash
java -jar target/jmeter-ai-0.2.0-SNAPSHOT.jar
```

### 5.3 接口调用 

创建自动化接口测试任务
**接口**: `POST /api/project/run`

**请求体 (JSON, 当不上传接口规范文档时)**:
```json
{
  "swaggerUrl": "http://localhost:8080/v3/api-docs",
  "programName": "电商后台管理系统",
  "extra": "重点测试订单创建接口的幂等性",
  "tags": ["user-controller", "order-controller"],
  "authorization": "Bearer <your_token_here>"
}
```

**请求方式 (Multipart/form-data, 当上传接口规范文档时)**:
*   接口地址: `/api/project/runWithSpec`
*   `request`: (Part, Application/JSON) 上述 JSON 对象
*   `specFile`: (Part, File) 接口规范 Markdown 文档 (.md)

*   `tags` (可选): 指定要测试的 Swagger Tags 列表。若为空或省略，则测试所有接口。
*   `authorization` (可选): 设置 Authorization 请求头的值。若提供，后续所有接口请求都会携带此 Header (Key: `Authorization`)。
*   `specFile` (可选): 上传接口规范文档（Markdown），系统将在生成测试用例时参考该文档中的业务约束和示例。

查询接口测试任务结果
**接口**: `GET /api/project/task/{taskId}`

**响应体**:
返回一个包含 `ProjectResult` 的大 JSON 对象，其中 `executionResults` 字段包含每个用例的详细执行与校验信息。

重新运行测试任务 (无 LLM)
**接口**: `POST /api/project/rerun/{taskId}`

**描述**:
基于指定的历史任务 ID，重新执行所有测试用例。此次执行完全在本地进行，使用初次运行时生成的断言规则进行校验，不消耗 LLM Token。

**响应体**:
```json
{
  "taskId": "new-uuid-...",
  "message": "Re-run started successfully"
}
```

## 6. 项目结构说明

```
jmeter-test/
├── doc/                        # 文档
├── src/main/java/com/example/jmeterai/
│   ├── JMeterAiApplication.java
│   ├── controller/
│   │   └── JMeterAiController.java  # 入口 API
│   ├── service/
│   │   ├── PipelineService.java     # [核心] 测试流程编排
│   │   ├── LlmService.java          # LLM SDK 封装
│   │   ├── CurlExecutorService.java # HTTP 请求执行
│   │   └── TestCaseGenerator.java   # 用例解析逻辑
│   ├── model/
│   │   ├── QualityScenario.java     # [新] 质量场景枚举
│   │   ├── TestCase.java            # 用例定义
│   │   ├── ExecutionResult.java     # 执行与校验结果
│   │   └── ...
│   └── util/
│       ├── OpenApiExtractor.java    # Swagger 解析
│       ├── PromptPresets.java       # [核心] 提示词库
│       └── ...
└── pom.xml
```

## 8. 核心工作流详解

本章节详细描述系统如何从零开始生成测试用例、执行测试并生成智能断言。该流程主要由 `PipelineService` 编排。

### 8.1 接口分析与场景遍历
1.  **接口提取**: 系统首先解析 Swagger/OpenAPI 文档，提取所有接口定义（Method, Path, Parameters, Body Schema）。
2.  **场景遍历**: 针对每一个接口，系统会依次遍历 `QualityScenario` 枚举中定义的所有质量场景（如 `HAPPY_PATH`, `PARAM_INTEGRITY`, `ABNORMAL_INPUT`）。

### 8.2 测试用例生成 (Test Case Generation)
*   **输入**: 
    *   接口的 OpenAPI 定义片段（JSON格式，包含完整的 Schema 定义）。
    *   当前测试场景说明（如“参数完整性：生成缺少必填参数的用例”）。
    *   **接口规范文档** (Optional): 如果用户上传了 Markdown 格式的接口规范，该文档的相关节选也会作为上下文输入给 LLM。
*   **LLM 交互**: 调用 `PromptPresets.singleInterfaceUserPrompt`，要求 LLM 生成符合该场景的一组测试用例。
    *   **Prompt 增强**: 针对手机号等特殊字段增加了强制随机生成规则；针对 Body 格式增加了强制字符串序列化要求。
*   **解析**: 使用 `TestCaseGenerator.parseLlmCases` 将 LLM 返回的 JSON 转换为 `TestCase` 对象列表。每个用例包含具体的 Headers, Query Params, Body 以及预期的测试目标 (`goal`)。

### 8.3 用例执行 (Execution)
*   系统使用 `CurlExecutorService` 将 `TestCase` 转换为实际的 HTTP 请求。
*   记录详细的执行结果 `ExecutionResult`，包括：HTTP 状态码、响应头、完整响应体、耗时等。

### 8.4 智能断言生成 (Assertion Generation)
这是本系统的核心亮点，不再依赖静态规则，而是根据**实际运行结果**动态生成断言。

1.  **输入**: 
    *   接口定义。
    *   测试用例详情（目标、输入参数）。
    *   **实际执行结果**（状态码、响应体）。
2.  **LLM 交互**: 
    *   调用 `PromptPresets.assertionGenerationUserPrompt`。
    *   **核心规则**: 即使 HTTP 状态码为 200，LLM 也必须检查响应体中的业务状态码（如 `code`, `success` 字段）。如果发现业务错误，必须生成断言来捕获该错误。
3.  **重试机制 (Retry Mechanism)**:
    *   系统会对 LLM 的输出进行严格的 JSON 解析。
    *   如果 LLM 返回格式错误、无法解析或未生成任何断言，系统会自动重试（默认最多 **3次**）。
    *   重试机制确保了断言生成的稳定性，避免因模型偶发波动导致测试“假通过”或“无断言”。
4.  **断言类型**:
    *   `statusCode`: 校验 HTTP 状态码。
    *   `jsonPath`: 使用 JsonPath 表达式校验特定字段（如 `$.code == 200`）。
    *   `bodyContains`: 校验响应体文本包含/不包含特定字符串。
    *   `responseTime`: 校验响应耗时。

### 8.5 本地验证 (Local Verification)
*   一旦断言生成成功，系统立即在本地运行这些断言 (`verifyLocally`)。
*   **判定规则**:
    *   只有当所有生成的断言都通过时，该用例才被标记为 `PASS`。
    *   如果有任何一个断言失败（例如 HTTP 200 但业务 Code 为 500），用例标记为 `FAIL`，并记录具体的失败原因。
    *   若经过多次重试仍未生成断言，系统将记录异常，防止测试被误判为成功。
